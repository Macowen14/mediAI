# MediAI - Medical RAG Assistant

> An intelligent medical question-answering system powered by Retrieval-Augmented Generation (RAG) for supporting medical professionals.

## üéØ Overview

MediAI is a production-ready medical AI assistant that combines vector database retrieval with large language models to provide accurate, evidence-based medical information. The system is designed to support medical professionals (medics, nurses, healthcare providers) with reliable medical knowledge retrieval and question answering.

**Key Features:**

- ü§ñ **Theme-Aware Response Generation** - Automatically classifies questions into 10 medical themes
- üìö **RAG Pipeline** - Retrieves relevant context from medical documents before generating answers
- üîç **Source Attribution** - Cites sources and indicates information provenance
- üìù **Comprehensive Logging** - Tracks model thinking and decision-making process
- ‚ö° **Batch Processing** - Efficiently handles multiple questions
- üé® **Theme-Specific Prompts** - Tailored responses for different medical question types

## üìã Table of Contents

- [Architecture](#architecture)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Project Structure](#project-structure)
- [Usage](#usage)
- [Testing](#testing)
- [Deployment](#deployment)
- [Configuration](#configuration)
- [Contributing](#contributing)

## üèóÔ∏è Architecture

```
User Question
     ‚Üì
Theme Detector (ministral-3:8b)
     ‚Üì
Vector Search (Pinecone)
     ‚Üì
Context Evaluation
     ‚Üì
Prompt Selection (Theme-specific)
     ‚Üì
Response Generation (deepseek-v3.1:671b-cloud)
     ‚Üì
Structured MedicalAnswer
```

### Core Components

- **Theme Detection**: Classifies questions into 10 medical categories (anatomy, physiology, pathology, pharmacology, symptoms, diagnosis, treatment, prevention, lifestyle, general)
- **Vector Database**: Pinecone for semantic search across medical documents
- **Embedding Model**: nomic-embed-text for document vectorization
- **Generation Models**: Ollama-powered LLMs for response generation
- **RAG Pipeline**: Orchestrates retrieval and generation workflow

## üì¶ Installation

### Prerequisites

- Python 3.11+
- [Ollama](https://ollama.ai/) installed and running
- Pinecone API key
- UV package manager (recommended)

### Required Ollama Models

Pull the required models:

```bash
ollama pull ministral-3:8b          # Theme detection
ollama pull deepseek-v3.1:671b-cloud # Main generation
ollama pull nomic-embed-text:latest  # Embeddings
```

### Setup

1. **Clone the repository:**

   ```bash
   git clone https://github.com/yourusername/mediAi.git
   cd mediAi
   ```

2. **Create virtual environment and install dependencies:**

   ```bash
   # Using UV (recommended)
   uv venv
   source .venv/bin/activate  # On Windows: .venv\Scripts\activate
   uv pip install -r requirements.txt

   # Or using pip
   python -m venv .venv
   source .venv/bin/activate
   pip install -r requirements.txt
   ```

3. **Configure environment variables:**

   ```bash
   cp .env.example .env
   # Edit .env and add your PINECONE_API_KEY
   ```

4. **Add medical documents:**
   ```bash
   # Place your medical PDF documents in the data/ directory
   cp /path/to/medical/pdfs/*.pdf data/
   ```

## üöÄ Quick Start

### Streamlit App (Interactive UI)

The easiest way to use the assistant:

```bash
streamlit run app.py
```

- **Real-time Streaming**: Watch the AI "think" and generate answers.
- **Model Selection**: Switch between available Ollama models.
- **Reference Sources**: View citations.

### Python API

```python
from src.rag_pipeline import Med icalRAGPipeline
from src.vector_utils import VectorStore, EmbeddingManager
from src.enums import ModelType
import os

# Initialize
embeddings = EmbeddingManager.get_embeddings(ModelType.EMBEDDING.value)
vectorstore = VectorStore.load_vectorstore(embeddings, "mediai-bot")
rag_pipeline = MedicalRAGPipeline(vectorstore)

# Ask a question
answer = rag_pipeline.process_question("What is hypertension?")

# Access results
print(f"Theme: {answer.theme}")
print(f"Answer: {answer.answer}")
print(f"Sources: {answer.sources}")
print(f"Confidence: {answer.confidence_score}")
```

### Batch Processing

```python
questions = [
    "What is diabetes?",
    "How does insulin work?",
    "What are symptoms of hypertension?"
]

answers = rag_pipeline.batch_process_questions(questions, search_k=3)

for q, a in zip(questions, answers):
    print(f"Q: {q}")
    print(f"A: {a.answer[:200]}...\n")
```

## üìÇ Project Structure

```
mediAi/
‚îú‚îÄ‚îÄ src/                        # Core source code
‚îÇ   ‚îú‚îÄ‚îÄ enums.py               # Enumerations (themes, models, sources)
‚îÇ   ‚îú‚îÄ‚îÄ models.py              # Pydantic data models
‚îÇ   ‚îú‚îÄ‚îÄ prompts.py             # Theme-specific prompt templates
‚îÇ   ‚îú‚îÄ‚îÄ logger.py              # Logging configuration
‚îÇ   ‚îú‚îÄ‚îÄ vector_utils.py        # Document processing & vector operations
‚îÇ   ‚îú‚îÄ‚îÄ model_utils.py         # Model management & inference
‚îÇ   ‚îî‚îÄ‚îÄ rag_pipeline.py        # Main RAG orchestrator
‚îú‚îÄ‚îÄ data/                       # Medical PDF documents
‚îú‚îÄ‚îÄ research/                   # Research notebooks
‚îÇ   ‚îî‚îÄ‚îÄ trials.ipynb           # Comprehensive testing notebook
‚îú‚îÄ‚îÄ logs/                       # Application logs
‚îú‚îÄ‚îÄ requirements.txt           # Python dependencies
‚îú‚îÄ‚îÄ .env                       # Environment configuration
‚îî‚îÄ‚îÄ README.md                  # This file
```

## üíª Usage

### Command Line Interface

```bash
# Run the Streamlit app (if available)
streamlit run app.py

# Or use the FastAPI backend
uvicorn api:app --reload
```

### Testing Notebook

The comprehensive testing notebook validates all functionality:

```bash
cd mediAi
jupyter notebook research/trials.ipynb
```

The notebook includes:

- ‚úÖ Environment setup and validation
- ‚úÖ Module import verification
- ‚úÖ Document loading and processing
- ‚úÖ Vector store initialization
- ‚úÖ Simple and complex question testing
- ‚úÖ Theme detection validation (all 10 themes)
- ‚úÖ Batch processing demonstration
- ‚úÖ Logging and performance metrics

## üß™ Testing

### Run the Comprehensive Test Suite

1. **Open the testing notebook:**

   ```bash
   jupyter notebook research/trials.ipynb
   ```

2. **Execute all cells sequentially**

3. **Review results:**
   - Simple question: "What is hypertension?"
   - Complex questions with multi-faceted medical topics
   - Theme detection accuracy across all categories
   - Batch processing performance
   - Logging output in `logs/mediai_YYYYMMDD.log`

### Expected Test Results

- ‚úÖ 8+ PDF documents loaded
- ‚úÖ 100+ document chunks created
- ‚úÖ Vector store successfully initialized
- ‚úÖ Theme detection >80% accuracy
- ‚úÖ Comprehensive medical answers with sources
- ‚úÖ Batch processing <5s per question
- ‚úÖ Complete logging of model thinking process

## üöÄ Deployment

### Production Deployment Checklist

1. **Environment Setup**
   - [ ] Configure production Pinecone index
   - [ ] Set up Ollama in production environment
   - [ ] Configure environment variables
   - [ ] Set up logging infrastructure

2. **API Deployment**
   - [ ] Create FastAPI endpoints
   - [ ] Implement rate limiting
   - [ ] Add authentication/authorization
   - [ ] Set up health check endpoints
   - [ ] Configure CORS policies

3. **Monitoring**
   - [ ] Set up metrics collection (Prometheus/Grafana)
   - [ ] Configure alerting for errors and latency
   - [ ] Implement distributed tracing
   - [ ] Track model performance metrics

4. **Scaling**
   - [ ] Implement response caching
   - [ ] Set up load balancing
   - [ ] Configure horizontal scaling
   - [ ] Optimize vector store queries

### Docker Deployment (Example)

```dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY src/ ./src/
COPY data/ ./data/

CMD ["uvicorn", "api:app", "--host", "0.0.0.0", "--port", "8000"]
```

## ‚öôÔ∏è Configuration

### Environment Variables

Create a `.env` file with:

```env
# Pinecone Configuration
PINECONE_API_KEY=your_pinecone_api_key_here

# Ollama Configuration (if remote)
OLLAMA_HOST=http://localhost:11434

# Logging Configuration
LOG_LEVEL=INFO
LOG_DIR=./logs

# Application Configuration
INDEX_NAME=mediai-bot
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
```

### Model Configuration

Edit `src/enums.py` to configure models:

```python
class ModelType(str, Enum):
    THEME_DETECTOR = "ministral-3:8b"
    MAIN_GENERATOR = "deepseek-v3.1:671b-cloud"
    EMBEDDING = "nomic-embed-text:latest"
```

## üìä Question Themes

The system automatically detects and routes questions to theme-specific prompts:

| Theme            | Description                     | Example Question                                       |
| ---------------- | ------------------------------- | ------------------------------------------------------ |
| **Anatomy**      | Body structure and anatomy      | "What is the structure of the human heart?"            |
| **Physiology**   | How body systems work           | "How does blood circulation work?"                     |
| **Pathology**    | Diseases and conditions         | "What is diabetes mellitus?"                           |
| **Pharmacology** | Medications and drugs           | "What is metformin used for?"                          |
| **Symptoms**     | Medical symptoms and signs      | "What causes chest pain?"                              |
| **Diagnosis**    | Diagnostic tests and procedures | "What does an ECG measure?"                            |
| **Treatment**    | Treatment options               | "What are treatments for hypertension?"                |
| **Prevention**   | Disease prevention              | "How can I prevent heart disease?"                     |
| **Lifestyle**    | Lifestyle and health habits     | "How does exercise affect health?"                     |
| **General**      | General medical questions       | "What's the difference between type 1 and 2 diabetes?" |

## üìù Logging

The system includes comprehensive logging that tracks:

- ‚úÖ Module initialization
- ‚úÖ Document processing steps
- ‚úÖ Vector search operations
- ‚úÖ **Theme detection reasoning**
- ‚úÖ **Model thinking and decision-making process**
- ‚úÖ Response generation details
- ‚úÖ Performance metrics
- ‚úÖ Error conditions

Logs are stored in `logs/mediai_YYYYMMDD.log`

## üîí Security & Compliance

### Important Disclaimers

> **‚ö†Ô∏è MEDICAL DISCLAIMER**: This system is designed to support medical professionals with information retrieval. It is NOT a substitute for professional medical advice, diagnosis, or treatment. Always consult qualified healthcare professionals for medical decisions.

### HIPAA Compliance

If handling Protected Health Information (PHI):

- Implement data encryption at rest and in transit
- Add comprehensive audit logging
- Implement access controls and authentication
- Ensure secure API endpoints
- Follow HIPAA guidelines for data handling

## ü§ù Contributing

Contributions are welcome! Please follow these guidelines:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- Medical documents from various open-source medical textbooks
- Powered by [Ollama](https://ollama.ai/) for local LLM inference
- Vector storage by [Pinecone](https://www.pinecone.io/)
- Built with [LangChain](https://www.langchain.com/)

## üìß Support

For support, please:

- Open an issue in the GitHub repository
- Check the [documentation](docs/)
- Review the [testing notebook](research/trials.ipynb) for examples

---

**MediAI** - Empowering medical professionals with AI-assisted knowledge retrieval üöÄ

Made with ‚ù§Ô∏è for the medical community
